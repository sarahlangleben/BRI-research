{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from cleantext import clean\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# url = \"https://eng.yidaiyilu.gov.cn/list/w/know_about\"\n",
    "def get_links_from_home(url):\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    # setup\n",
    "    all_links = soup.ul\n",
    "    # all the links in the <ul> tag on the yidaiyilu site. This contains all the sources they link in their online list. \n",
    "    all_links = str(all_links)\n",
    "    # turning into a string object instead of a bs4 object. \n",
    "    all_content_list = all_links.split(\" \")\n",
    "    # splitting based on spaces, this is just to make it into a legible list. \n",
    "    links_list = []\n",
    "    i = 0\n",
    "    for value in all_content_list: \n",
    "        # looping through the list, building a new list with only the links. \n",
    "        if \"href\" in value: \n",
    "            links_list.append(value)\n",
    "    links_cleaned_list = []\n",
    "    for value in links_list: \n",
    "        # cleaning the links-- if it doesn't have a full https in front of it already adding it, removing the html href tag. \n",
    "        if \"https://eng.yidaiyilu.gov.cn\" not in value:\n",
    "            full_link = \"https://eng.yidaiyilu.gov.cn\"+value[5:]\n",
    "            links_cleaned_list.append(full_link.replace('\"', ''))\n",
    "        if value[-1] != \"l\":\n",
    "            value+\"l\"\n",
    "        else:\n",
    "            links_cleaned_list.append(value[5:])\n",
    "    print(links_cleaned_list)\n",
    "    return(links_cleaned_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(original_title, file_use):\n",
    "  \"\"\"\n",
    "  helper to clean up the title for usage in creating the .txt file\n",
    "  \"\"\"\n",
    "  list_title = list(str(original_title))\n",
    "  starting_point = list_title.index(\">\")\n",
    "  stopping_point = list_title.index(\"<\", 59)\n",
    "  output = \"\"\n",
    "  for x in range(starting_point+10, stopping_point):\n",
    "    output= output + list_title[x]\n",
    "    x+=1\n",
    "  if file_use:  \n",
    "    output = output.replace(\" \", \"\")\n",
    "    output = output.replace('\\n', \"\")\n",
    "    final_output = ''.join(letter for letter in output if letter.isalnum())\n",
    "    return str(final_output)\n",
    "  else: \n",
    "      output = output.replace('\\n', \"\")\n",
    "      return str(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(all_text): \n",
    "  \"\"\"\n",
    "  Fixing up the Updated, Source fields\n",
    "  all_text is all of text content from webpage. Should be a string.\n",
    "  Use to find the Source, Updated, and Editor fields in the article.\n",
    "  \"\"\"\n",
    "  for x in range(len(all_text)):\n",
    "    if \"Source:\" in all_text[x]: return(all_text[x]+ \" \"+ all_text[x+1] + \"\\n\")\n",
    "\n",
    "def get_editor(all_text):\n",
    "  \"\"\"\n",
    "  getting the editor name\n",
    "  \"\"\"\n",
    "  for x in range(len(all_text)):\n",
    "    if \"Editor:\" in all_text[x]: return(all_text[x]+\" \"+all_text[x+1] + \"\\n\")\n",
    "\n",
    "def get_updated(all_text):\n",
    "  \"\"\"\n",
    "  getting the last updated timestamp\n",
    "  \"\"\"\n",
    "  for x in range(len(all_text)):\n",
    "    if \"Updated:\" in all_text[x]: return(all_text[x]+ \" \"+all_text[x+1]+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_cleanup(article_content):\n",
    "  \"\"\"\n",
    "  cleaning up the <p> content\n",
    "  1. turn article_content into a list\n",
    "  2. turn list contents into strings\n",
    "  3. if not in there, write it into file, else don't.\n",
    "\n",
    "  \"\"\"\n",
    "  # print(article_content)\n",
    "  article_content_list = list(article_content)\n",
    "\n",
    "  for content in article_content_list: \n",
    "    content = str(content)\n",
    "    if(\"data\" not in content):\n",
    "      content = content.replace(\"<p>\", \"\").replace(\"</p>\", \"\")\n",
    "      return(content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the raw text data\n",
    "def get_raw_data(link):\n",
    "  \"\"\"\"\n",
    "  Setup \n",
    "  \"\"\"\n",
    "  # f = open(\"demo.txt\", \"w\")\n",
    "  # link = \"https://eng.yidaiyilu.gov.cn/p/0H647R18.html\"\n",
    "  link_req = requests.get(link).text.encode(\"utf8\").decode(\"ascii\", \"ignore\")\n",
    "  page_soup = BeautifulSoup(link_req, \"html.parser\")\n",
    "  article_content = page_soup.find_all(\"p\")\n",
    "  article_content = re.sub(\"<[^>]+>\", '', str(article_content))\n",
    "  article_content = article_content.replace(\"[\", '')\n",
    "  article_content = article_content.replace(\"]\", '')\n",
    "  # print(article_content)\n",
    "  # everything with a <p> tag\n",
    "  all_text = page_soup.find_all(string = True)\n",
    "  # all the string content; useful for getting the date updated, source, editor\n",
    "  title = page_soup.find_all(\"h1\", string=True)\n",
    "  filename = clean_title(title, True)+\".txt\"\n",
    "  # getting a name for the file\n",
    "  f = open(filename, \"w\")\n",
    "  f.write(link + \"\\n\")\n",
    "  f.write(clean_title(title, False) + \"\\n\")\n",
    "  f.write(get_source(all_text))\n",
    "  f.write(get_editor(all_text).strip()+\"\\n\")\n",
    "  f.write(get_updated(all_text))\n",
    "  f.write(paragraph_cleanup(article_content)+\"\\n\")\n",
    "  f.write(article_content)\n",
    "  # getting the title to write into the file\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://eng.yidaiyilu.gov.cn/p/0H647R18.html', 'https://eng.yidaiyilu.gov.cn/p/0PPN0HMN.html', 'https://eng.yidaiyilu.gov.cn/p/0N4P7CF0.html', 'https://eng.yidaiyilu.gov.cn/p/006Q9GBH.html', 'https://eng.yidaiyilu.gov.cn/p/0CESAH1N.html', 'https://eng.yidaiyilu.gov.cn/p/041D0RIJ.html', 'https://eng.yidaiyilu.gov.cn/p/86739.html', 'https://eng.yidaiyilu.gov.cn/p/46076.html', 'https://eng.yidaiyilu.gov.cn/p/34863.html', 'https://eng.yidaiyilu.gov.cn/p/30277.html', 'https://eng.yidaiyilu.gov.cn/p/16639.html', 'https://eng.yidaiyilu.gov.cn/p/13754.html', 'https://eng.yidaiyilu.gov.cn/p/12731.html', 'https://eng.yidaiyilu.gov.cn/p/12479.html', 'https://eng.yidaiyilu.gov.cn/p/10477.html', 'https://eng.yidaiyilu.gov.cn/p/1084.html']\n"
     ]
    }
   ],
   "source": [
    "# for each link:\n",
    "# \n",
    "# 1. create a new file (open())\n",
    "# 2. write the text content into the file\n",
    "# 3. close the file.\n",
    "\n",
    "def mover():\n",
    "  links = get_links_from_home(\"https://eng.yidaiyilu.gov.cn/list/w/know_about\")\n",
    "  for link in links:\n",
    "    get_raw_data(link)\n",
    "    \n",
    "mover()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
