{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://eng.yidaiyilu.gov.cn/p/0H647R18.html', 'https://eng.yidaiyilu.gov.cn/p/0PPN0HMN.html', 'https://eng.yidaiyilu.gov.cn/p/0N4P7CF0.html', 'https://eng.yidaiyilu.gov.cn/p/006Q9GBH.html', 'https://eng.yidaiyilu.gov.cn/p/0CESAH1N.html', 'https://eng.yidaiyilu.gov.cn/p/041D0RIJ.html', 'https://eng.yidaiyilu.gov.cn/p/86739.html', 'https://eng.yidaiyilu.gov.cn/p/46076.html', 'https://eng.yidaiyilu.gov.cn/p/34863.html', 'https://eng.yidaiyilu.gov.cn/p/30277.html', 'https://eng.yidaiyilu.gov.cn/p/16639.html', 'https://eng.yidaiyilu.gov.cn/p/13754.html', 'https://eng.yidaiyilu.gov.cn/p/12731.html', 'https://eng.yidaiyilu.gov.cn/p/12479.html', 'https://eng.yidaiyilu.gov.cn/p/10477.html', 'https://eng.yidaiyilu.gov.cn/p/1084.html']\n"
     ]
    }
   ],
   "source": [
    "url = \"https://eng.yidaiyilu.gov.cn/list/w/know_about\"\n",
    "req = requests.get(url)\n",
    "soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "# setup\n",
    "all_links = soup.ul\n",
    "# all the links in the <ul> tag on the yidaiyilu site. This contains all the sources they link in their online list. \n",
    "all_links = str(all_links)\n",
    "# turning into a string object instead of a bs4 object. \n",
    "all_content_list = all_links.split(\" \")\n",
    "# splitting based on spaces, this is just to make it into a legible list. \n",
    "links_list = []\n",
    "i = 0\n",
    "for value in all_content_list: \n",
    "    # looping through the list, building a new list with only the links. \n",
    "    if \"href\" in value: \n",
    "        links_list.append(value)\n",
    "links_cleaned_list = []\n",
    "for value in links_list: \n",
    "    # cleaning the links-- if it doesn't have a full https in front of it already adding it, removing the html href tag. \n",
    "    if \"https://eng.yidaiyilu.gov.cn\" not in value:\n",
    "        full_link = \"https://eng.yidaiyilu.gov.cn\"+value[5:]\n",
    "        links_cleaned_list.append(full_link.replace('\"', ''))\n",
    "    if value[-1] != \"l\":\n",
    "        value+\"l\"\n",
    "    else:\n",
    "        links_cleaned_list.append(value[5:])\n",
    "print(links_cleaned_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the raw text data\n",
    "# f = open(\"demo.txt\", \"w\")\n",
    "\"\"\"\"\n",
    "setup\n",
    "\"\"\"\n",
    "link = \"https://eng.yidaiyilu.gov.cn/p/0H647R18.html\"\n",
    "link_req = requests.get(link).text.encode(\"utf8\").decode(\"ascii\", \"ignore\")\n",
    "page_soup = BeautifulSoup(link_req, \"html.parser\")\n",
    "\n",
    "article_content = page_soup.find_all(\"p\")\n",
    "# everything with a <p> tag\n",
    "all_text = page_soup.find_all(string = True)\n",
    "# all the string content; useful for getting the date updated, source, editor\n",
    "title = page_soup.find_all(\"h1\")\n",
    "\n",
    "# f.write(str(link))\n",
    "# f.write(str(article_content))\n",
    "# f = open(\"title.txt\", \"w\")\n",
    "# f.write(str(text))\n",
    "# all_text\n",
    "# title\n",
    "# title=str(title).split(\"<\" or \">\")\n",
    "# title=list(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: June 26, 2023\n",
      "Source: Xinhua Silk Road \n",
      "\n",
      "        Editor:Hu Pingchao\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fixing up the Updated, Source fields\n",
    "\"\"\"\n",
    "for x in range(len(all_text)):\n",
    "  if \"Source:\" in all_text[x]:print(all_text[x] + \" \"+ all_text[x+1])\n",
    "    # f.write(str(all_text[x]+ \" \"+ all_text[x+1]))\n",
    "    # print(all_text[x] + \" \"+ all_text[x+1])\n",
    "  elif \"Updated:\" in all_text[x]: print(all_text[x]+ \" \"+all_text[x+1])\n",
    "  elif \"Editor:\" in all_text[x]: print(all_text[x]+\" \"+all_text[x+1])\n",
    "    # f.write(str(all_text[x]+\" \"+all_text[x+1]))\n",
    "    # print(all_text[x]+ \" \"+all_text[x+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncleaning up the <p> content\\n'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cleaning up the <p> content\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_files_txt(input_list):\n",
    "  for value in input_list: \n",
    "    link = value \n",
    "    link_req = requests.get(link)\n",
    "    page_soup = BeautifulSoup(link_req.text, \"html.parser\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
